{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93175ca9",
   "metadata": {},
   "source": [
    "# GANDhani: CycleGAN for Cultural Style Transfer Translating Bandhani Textile Motifs onto Contemporary Apparel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d746b4",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade7901",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_grid, save_image\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode, functional \u001b[38;5;28;01mas\u001b[39;00m TF\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision.transforms import InterpolationMode, functional as TF\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a0b90",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438822b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data directories\n",
    "base = pathlib.Path('./data')\n",
    "bandhani_dir = base / 'bandhani'\n",
    "modern_dir = base / 'modern'\n",
    "\n",
    "\n",
    "bandhani_dir.mkdir(parents=True, exist_ok=True)\n",
    "modern_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9449df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download & unzip into those dirs\n",
    "\n",
    "api.dataset_download_files(\n",
    "    \"amansunesh/bandhani-design-dataset\",\n",
    "    path=str(bandhani_dir),\n",
    "    unzip=True\n",
    ")\n",
    "\n",
    "api.dataset_download_files(\n",
    "    \"amansunesh/modern-art-dataset\",\n",
    "    path=str(modern_dir),\n",
    "    unzip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7e0c1",
   "metadata": {},
   "source": [
    "### Data Proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(286, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.RandomCrop(256),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),  (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7383364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None):\n",
    "        base = pathlib.Path(root_dir) / mode\n",
    "        self.bandhani_paths = sorted((base / \"bandhani\").glob(\"*.jpg\"))\n",
    "        self.modern_paths   = sorted((base / \"modern\").glob(\"*.jpg\"))\n",
    "\n",
    "        assert len(self.bandhani_paths) and len(self.modern_paths), \\\n",
    "            f\"No images found in {base}/bandhani or {base}/modern\"\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.bandhani_paths), len(self.modern_paths))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_A = self.bandhani_paths[idx]\n",
    "        path_B = self.modern_paths[idx]\n",
    "\n",
    "        img_A = Image.open(path_A).convert(\"RGB\")\n",
    "        img_B = Image.open(path_B).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "            img_B = self.transform(img_B)\n",
    "\n",
    "        return img_A, img_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d467fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "root_dir = \"./data\"\n",
    "\n",
    "train_ds = CycleGANDataset(root_dir, mode = 'train', transform = train_transform)\n",
    "train_dl = DataLoader(train_ds, batch_size = bs, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize from [-1,1] back to [0,1]\n",
    "def denorm(imgs):\n",
    "    return imgs * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b47552",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandhani, modern = train_ds[0]\n",
    "\n",
    "# Un‐normalize and convert to numpy\n",
    "bandhani_np = denorm(bandhani).permute(1,2,0).cpu().numpy()\n",
    "modern_np   = denorm(modern).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "for ax, img, title in zip(axes, (bandhani_np, modern_np), ('Bandhani','Modern')):\n",
    "    ax.imshow(img); ax.set_title(title); ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6faa883",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d99da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_features, base_features * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(base_features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(base_features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(base_features * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_features * 8, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a92a0f",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf8a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x) # Skip Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba419f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.g1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, features, kernel_size=7, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.g2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.g3 = nn.Sequential(\n",
    "            nn.Conv2d(features*2, features*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        res_blocks = []\n",
    "\n",
    "        for _ in range(9):\n",
    "            res_blocks.append(ResidualBlock(features*4))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "\n",
    "        self.g4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*4, features*2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.g5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.g6 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(features, out_channels, kernel_size=7, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        g1 = self.g1(x)\n",
    "        g2 = self.g2(g1)\n",
    "        g3 = self.g3(g2)\n",
    "        res = self.res_blocks(g3)\n",
    "        g4 = self.g4(res)\n",
    "        g5 = self.g5(g4)\n",
    "        \n",
    "        return self.g6(g5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db5686",
   "metadata": {},
   "source": [
    "### Discriminator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator_A, discriminator_B,\n",
    "                        generator_A, generator_B,\n",
    "                        real_A, real_B, \n",
    "                        fake_A, fake_B,\n",
    "                        opt_d, criterion_GAN):\n",
    "    \n",
    "    discriminator_A.train()\n",
    "    discriminator_B.train()\n",
    "\n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # --- Train D_A ---\n",
    "\n",
    "    # ——— Real pairs ———\n",
    "    # D(map, real) should predict “real” → target=1\n",
    "    real_preds_A = discriminator_A(real_A)\n",
    "    real_targets_A = torch.ones_like(real_preds_A)\n",
    "    loss_D_A_real = criterion_GAN(real_preds_A, real_targets_A)\n",
    "    real_score_A = real_preds_A.mean().item()\n",
    "\n",
    "    # ——— Fake pairs ———\n",
    "    # Generate fake images\n",
    "    # G(map) → fake; detach so G’s grad isn’t updated here\n",
    "    fake_preds_A = discriminator_A(fake_A.detach())\n",
    "    fake_targets_A = torch.zeros_like(fake_preds_A)\n",
    "    loss_D_A_fake = criterion_GAN(fake_preds_A, fake_targets_A)\n",
    "    fake_score_A  = fake_preds_A.mean().item()\n",
    "\n",
    "    loss_D_A = 0.5 * (loss_D_A_real + loss_D_A_fake)\n",
    "\n",
    "\n",
    "    # --- Train D_B ---\n",
    "\n",
    "    # ——— Real pairs ———\n",
    "    # D(map, real) should predict “real” → target=1\n",
    "    real_preds_B = discriminator_B(real_B)\n",
    "    real_targets_B = torch.ones_like(real_preds_B)\n",
    "    loss_D_B_real = criterion_GAN(real_preds_B, real_targets_B)\n",
    "    real_score_B = real_preds_B.mean().item()\n",
    "\n",
    "    # ——— Fake pairs ———\n",
    "    # Generate fake images\n",
    "    # G(map) → fake; detach so G’s grad isn’t updated here\n",
    "    fake_preds_B = discriminator_B(fake_B.detach())\n",
    "    fake_targets_B = torch.zeros_like(fake_preds_B)\n",
    "    loss_D_B_fake = criterion_GAN(fake_preds_B, fake_targets_B)\n",
    "    fake_score_B  = fake_preds_B.mean().item()\n",
    "\n",
    "    loss_D_B = 0.5 * (loss_D_B_real + loss_D_B_fake)\n",
    "\n",
    "\n",
    "    # --- Total Discriminator Loss ---\n",
    "    loss_D = loss_D_A + loss_D_B\n",
    "    loss_D.backward()\n",
    "    opt_d.step()\n",
    "    \n",
    "    return {\n",
    "        'loss_D_A': loss_D_A.item(),\n",
    "        'loss_D_B': loss_D_B.item(),\n",
    "        'real_A_score': real_score_A,\n",
    "        'fake_A_score': fake_score_A,\n",
    "        'real_B_score': real_score_B,\n",
    "        'fake_B_score': fake_score_B\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d80fe4",
   "metadata": {},
   "source": [
    "### Generator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5679633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(discriminator_A, discriminator_B,\n",
    "                        generator_A, generator_B,\n",
    "                        real_A, real_B, \n",
    "                        lambda_a, lambda_b, lambda_id,\n",
    "                        opt_g, criterion_GAN):\n",
    "    \n",
    "    # Clear generator gradients                                     \n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    fake_B = generator_A(real_A)\n",
    "    fake_A = generator_B(real_B)\n",
    "\n",
    "    # --- Train D_A ---\n",
    "\n",
    "    # 1) Adverserial Loss\n",
    "    preds_fake_a = discriminator_A(fake_A)\n",
    "    targets_a = torch.ones_like(preds_fake_a)\n",
    "    adv_total_b = criterion_GAN(preds_fake_a, targets_a)\n",
    "\n",
    "    preds_fake_b = discriminator_B(fake_B)\n",
    "    targets_b = torch.ones_like(preds_fake_b)\n",
    "    adv_total_a = criterion_GAN(preds_fake_b, targets_b)\n",
    "\n",
    "    # 2) Cycle Losses\n",
    "    rec_A = generator_B(fake_B)\n",
    "    rec_B = generator_A(fake_A)\n",
    "\n",
    "    loss_cycle_A = F.l1_loss(rec_A, real_A)\n",
    "    loss_cycle_B = F.l1_loss(rec_B, real_B)\n",
    "\n",
    "    # 3) Identity Loss\n",
    "    idt_B = generator_A(real_B)\n",
    "    idt_A = generator_B(real_A)\n",
    "    identity_a = F.l1_loss(idt_B, real_B)\n",
    "    identity_b = F.l1_loss(idt_A, real_A)\n",
    "\n",
    "    loss_a = adv_total_a + (lambda_a * loss_cycle_A) + (lambda_id * lambda_b * identity_a)\n",
    "    loss_b = adv_total_b + (lambda_b * loss_cycle_B) + (lambda_id * lambda_a * identity_b)\n",
    "\n",
    "    loss_total = loss_a + loss_b\n",
    "    \n",
    "    loss_total.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return {\n",
    "        'loss_total': loss_total.item(),\n",
    "        'G_A_loss':   loss_a.item(),\n",
    "        'G_B_loss':   loss_b.item(),\n",
    "        'adv_A':      adv_total_a.item(),\n",
    "        'adv_B':      adv_total_b.item(),\n",
    "        'cycle_A':    loss_cycle_A.item(),\n",
    "        'cycle_B':    loss_cycle_B.item(),\n",
    "        'idt_A':      identity_a.item(),\n",
    "        'idt_B':      identity_b.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f5c1d",
   "metadata": {},
   "source": [
    "### Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize from [-1,1] back to [0,1]\n",
    "def denorm(imgs):\n",
    "    return imgs * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cycle_samples(\n",
    "    epoch: int,\n",
    "    real_A: torch.Tensor,\n",
    "    real_B: torch.Tensor,\n",
    "    generator_A: nn.Module,\n",
    "    generator_B: nn.Module,\n",
    "    denorm,\n",
    "    sample_dir: str = \"generated\",\n",
    "    nrow: int = 8\n",
    "):\n",
    "    generator_A.eval()\n",
    "    generator_B.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_B = generator_A(real_A.to(next(generator_A.parameters()).device))\n",
    "        fake_A = generator_B(real_B.to(next(generator_B.parameters()).device))\n",
    "\n",
    "    # bring back to [0,1]\n",
    "    real_A_vis = denorm(real_A.cpu())\n",
    "    real_B_vis = denorm(real_B.cpu())\n",
    "    fake_A_vis = denorm(fake_A.cpu())\n",
    "    fake_B_vis = denorm(fake_B.cpu())\n",
    "\n",
    "    # make grids\n",
    "    grid_A2B = make_grid(\n",
    "        torch.cat([real_A_vis, fake_B_vis], dim=0),\n",
    "        nrow=nrow,\n",
    "        padding=2,\n",
    "        normalize=False\n",
    "    )\n",
    "    grid_B2A = make_grid(\n",
    "        torch.cat([real_B_vis, fake_A_vis], dim=0),\n",
    "        nrow=nrow,\n",
    "        padding=2,\n",
    "        normalize=False\n",
    "    )\n",
    "\n",
    "    # save\n",
    "    save_image(grid_A2B, os.path.join(sample_dir, f\"A2B_epoch{epoch:03d}.png\"))\n",
    "    save_image(grid_B2A, os.path.join(sample_dir, f\"B2A_epoch{epoch:03d}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ace7dc",
   "metadata": {},
   "source": [
    "### Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24add8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def push_and_pop(self, images):\n",
    "        out = []\n",
    "        for img in images:\n",
    "            img = img.unsqueeze(0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(img)\n",
    "                out.append(img)\n",
    "            else:\n",
    "                if random.random() < 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    out.append(self.data[i].clone())\n",
    "                    self.data[i] = img\n",
    "                else:\n",
    "                    out.append(img)\n",
    "        return torch.cat(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163e9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    # 1.0 for epoch ∈ [0, epochs/2), then linearly to 0 by epoch=epochs\n",
    "    return 1.0 - max(0, epoch - epochs//2) / float(epochs//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if hasattr(m, \"weight\") and m.weight is not None:\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            \n",
    "    if hasattr(m, \"bias\") and m.bias is not None:\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07a047",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3572880838.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef fit()\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def fit(\n",
    "    discriminator_A: nn.Module,\n",
    "    discriminator_B: nn.Module,\n",
    "    generator_A:     nn.Module,\n",
    "    generator_B:     nn.Module,\n",
    "    train_dl:        DataLoader,\n",
    "    denorm,\n",
    "    device:          torch.device,\n",
    "    epochs:          int      = 200,\n",
    "    lr:              float    = 2e-4,\n",
    "    lambda_a:        float    = 10.0,\n",
    "    lambda_b:        float    = 10.0,\n",
    "    lambda_id:       float    = 0.5,\n",
    "    pool_size:       int      = 50,\n",
    "    sample_dir:      str      = \"generated\",\n",
    "    nrow:            int      = 8\n",
    "):\n",
    "    opt_G = torch.optim.Adam(\n",
    "        list(generator_A.parameters()) + list(generator_B.parameters()),\n",
    "        lr=lr, betas=(0.5, 0.999)\n",
    "    )\n",
    "    opt_D = torch.optim.Adam(\n",
    "        list(discriminator_A.parameters()) + list(discriminator_B.parameters()),\n",
    "        lr=lr, betas=(0.5, 0.999)\n",
    "    )\n",
    "\n",
    "    sched_G = torch.optim.lr_scheduler.LambdaLR(opt_G, lr_lambda=lambda_rule)\n",
    "    sched_D = torch.optim.lr_scheduler.LambdaLR(opt_D, lr_lambda=lambda_rule)\n",
    "\n",
    "    # grab one fixed batch for visualization \n",
    "    fixed_A, fixed_B = next(iter(train_dl))\n",
    "    fixed_A, fixed_B = fixed_A.to(device), fixed_B.to(device)\n",
    "\n",
    "    criterion_GAN = nn.MSELoss() # LSGAN\n",
    "    buffer_A = ReplayBuffer(pool_size)  # for fake_A\n",
    "    buffer_B = ReplayBuffer(pool_size)  # for fake_B\n",
    "\n",
    "    # ——— history ———\n",
    "    history = {\n",
    "        'G_A': [], 'G_B': [],\n",
    "        'D_A': [], 'D_B': [],\n",
    "        'cycle_A': [], 'cycle_B': [],\n",
    "        'idt_A': [],   'idt_B': [],\n",
    "        'adv_A': [],   'adv_B': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for real_A, real_B in train_dl:\n",
    "            real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "\n",
    "            gen_metrics = train_generator(\n",
    "                discriminator_A, discriminator_B,\n",
    "                generator_A, generator_B,\n",
    "                real_A, real_B,\n",
    "                lambda_a, lambda_b, lambda_id,\n",
    "                opt_G, criterion_GAN\n",
    "            )\n",
    "            \n",
    "            # produce fresh fakes (for the buffer)\n",
    "            fake_B = generator_A(real_A).detach()\n",
    "            fake_A = generator_B(real_B).detach()\n",
    "\n",
    "            # ——— 2) Discriminators ———\n",
    "            # pull from buffer\n",
    "            fake_A_buf = buffer_A.push_and_pop(fake_A)\n",
    "            fake_B_buf = buffer_B.push_and_pop(fake_B)\n",
    "\n",
    "            disc_metrics = train_discriminator(\n",
    "                discriminator_A, discriminator_B,\n",
    "                real_A, real_B,\n",
    "                fake_A_buf, fake_B_buf,\n",
    "                opt_D, criterion_GAN\n",
    "            )\n",
    "\n",
    "            # Log losses & scores\n",
    "            history['G_A'].append(gen_metrics['G_A_loss'])\n",
    "            history['G_B'].append(gen_metrics['G_B_loss'])\n",
    "            history['adv_A'].append(gen_metrics['adv_A'])\n",
    "            history['adv_B'].append(gen_metrics['adv_B'])\n",
    "            history['cycle_A'].append(gen_metrics['cycle_A'])\n",
    "            history['cycle_B'].append(gen_metrics['cycle_B'])\n",
    "            history['idt_A'].append(gen_metrics['idt_A'])\n",
    "            history['idt_B'].append(gen_metrics['idt_B'])\n",
    "\n",
    "            history['D_A'].append(disc_metrics['loss_D_A'])\n",
    "            history['D_B'].append(disc_metrics['loss_D_B'])\n",
    "\n",
    "        # Step the schedulers each epoch\n",
    "        sched_G.step()\n",
    "        sched_D.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(generator_A.state_dict(), f\"checkpoint_gen_a_epoch{epoch}.pth\")\n",
    "            torch.save(generator_B.state_dict(), f\"checkpoint_gen_b_epoch{epoch}.pth\")\n",
    "\n",
    "        # every epoch dump sample grids A→B and B→A\n",
    "        save_cycle_samples(\n",
    "            epoch,\n",
    "            fixed_A, fixed_B,\n",
    "            generator_A, generator_B,\n",
    "            denorm,\n",
    "            sample_dir=sample_dir,\n",
    "            nrow=nrow\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs}  \"\n",
    "              f\"G_A: {gen_metrics['G_A_loss']:.3f}, \"\n",
    "              f\"G_B: {gen_metrics['G_B_loss']:.3f}, \"\n",
    "              f\"D_A: {disc_metrics['loss_D_A']:.3f}, \"\n",
    "              f\"D_B: {disc_metrics['loss_D_B']:.3f}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66adb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "discriminator_a = Discriminator().to(device)\n",
    "discriminator_b = Discriminator().to(device)\n",
    "generator_a     = Generator().to(device)\n",
    "generator_b     = Generator().to(device)\n",
    "\n",
    "generator_a.apply(init_weights)\n",
    "generator_b.apply(init_weights)\n",
    "discriminator_a.apply(init_weights)\n",
    "discriminator_b.apply(init_weights)\n",
    "\n",
    "history = fit(\n",
    "    discriminator_a=discriminator_a,\n",
    "    discriminator_b=discriminator_b,\n",
    "    generator_a=generator_a,\n",
    "    generator_b=generator_b,\n",
    "    train_dl=train_dl,\n",
    "    denorm=denorm,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdabeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_G_A = history['G_A']\n",
    "losses_G_B = history['G_B']\n",
    "losses_D_A = history['D_A']\n",
    "losses_D_B = history['D_B']\n",
    "adv_A      = history['adv_A']\n",
    "adv_B      = history['adv_B']\n",
    "cycle_A    = history['cycle_A']\n",
    "cycle_B    = history['cycle_B']\n",
    "idt_A      = history['idt_A']\n",
    "idt_B      = history['idt_B']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa3c7a",
   "metadata": {},
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(generator_a.state_dict(), 'G_a.pth')\n",
    "torch.save(generator_b.state_dict(), 'G_b.pth')\n",
    "torch.save(discriminator_a.state_dict(), 'D_a.pth')\n",
    "torch.save(discriminator_b.state_dict(), 'D_b.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ef3d8",
   "metadata": {},
   "source": [
    "Here's how the generated images look, after the 1st, 10th, 50th, 100th, 150th, 200th, 300th epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f5fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff76b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a6a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00b0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e08d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a65ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df732a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b9060bc",
   "metadata": {},
   "source": [
    "### Plotting Loss of Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = list(range(1, len(losses_D_A) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, losses_D_A, label=\"Discriminator_A\")\n",
    "plt.plot(epochs_range, losses_G_A, label=\"Generator_A\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, losses_D_B, label=\"Discriminator_B\")\n",
    "plt.plot(epochs_range, losses_G_B, label=\"Generator_B\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33510c7",
   "metadata": {},
   "source": [
    "### Plotting Adverserial Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37851adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, adv_A, label =\"Adv Loss A→B\")\n",
    "plt.plot(epochs_range, adv_B, label=\"Adv Loss B→A\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Adversarial Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fa7cd",
   "metadata": {},
   "source": [
    "### Plotting Cycle-Consistency Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, cycle_A, label =\"Cycle Loss A→A\")\n",
    "plt.plot(epochs_range, cycle_B, label=\"Cycle Loss B→B\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cycle-consistency Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369aefb",
   "metadata": {},
   "source": [
    "### Plotting Identity Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(epochs_range, idt_A, label=\"Identity Loss A\", linewidth=2)\n",
    "plt.plot(epochs_range, idt_B, label=\"Identity Loss B\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Identity Loss\")\n",
    "plt.title(\"Identity Losses over Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "# Primary axis: cycle losses (often larger)\n",
    "ax1.plot(epochs_range, cycle_A,  'C1-', label=\"Cycle Loss A\")\n",
    "ax1.plot(epochs_range, cycle_B,  'C2-', label=\"Cycle Loss B\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Cycle Loss\")\n",
    "\n",
    "# Secondary axis: identity losses (much smaller)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs_range, idt_A,   'C3--', label=\"Identity Loss A\")\n",
    "ax2.plot(epochs_range, idt_B,   'C4--', label=\"Identity Loss B\")\n",
    "ax2.set_ylabel(\"Identity Loss\")\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Cycle vs. Identity Loss (A & B)\")\n",
    "plt.grid(True, which=\"both\", axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6762d7",
   "metadata": {},
   "source": [
    "###  Generating New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = CycleGANDataset(root_dir='data', mode='val', transform=train_transform)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gen_A2B = Generator().to(device)\n",
    "gen_B2A = Generator().to(device)\n",
    "gen_A2B.load_state_dict(torch.load(''))\n",
    "gen_B2A.load_state_dict(torch.load(''))\n",
    "gen_A2B.eval()\n",
    "gen_B2A.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91761290",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A, real_B = next(iter(val_dl))\n",
    "real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_B = gen_A2B(real_A)\n",
    "    fake_A = gen_B2A(real_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef95a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A = denorm(real_A)[:16]\n",
    "fake_B = denorm(fake_B)[:16]\n",
    "real_B = denorm(real_B)[:16]\n",
    "fake_A = denorm(fake_A)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleave Real vs. Fake pairs for each domain\n",
    "# Resulting shape = [32, C, H, W]\n",
    "pairs_A2B = torch.stack([real_A, fake_B], dim=1).reshape(-1, *real_A.shape[1:])\n",
    "pairs_B2A = torch.stack([real_B, fake_A], dim=1).reshape(-1, *real_B.shape[1:])\n",
    "\n",
    "# Make grids (8 columns × 4 rows)\n",
    "grid_A2B = make_grid(pairs_A2B, nrow=8, padding=2)\n",
    "grid_B2A = make_grid(pairs_B2A, nrow=8, padding=2)\n",
    "\n",
    "# Plot side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# A -> B\n",
    "axes[0].imshow(grid_A2B.permute(1, 2, 0).cpu().numpy())\n",
    "axes[0].set_title(\"A → B: [Real A, Fake B] pairs\", fontsize=16)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# B -> A\n",
    "axes[1].imshow(grid_B2A.permute(1, 2, 0).cpu().numpy())\n",
    "axes[1].set_title(\"B → A: [Real B, Fake A] pairs\", fontsize=16)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982ec08",
   "metadata": {},
   "source": [
    "### User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898df565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_A2B(img: Image.Image) -> Image.Image:\n",
    "    x = train_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = gen_A2B(x)\n",
    "\n",
    "    out = denorm(fake[0]).clamp(0,1)\n",
    "\n",
    "    return transforms.ToPILImage()(out.cpu())\n",
    "\n",
    "\n",
    "def translate_B2A(img: Image.Image) -> Image.Image:\n",
    "    x = train_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = gen_B2A(x)\n",
    "\n",
    "    out = denorm(fake[0]).clamp(0,1)\n",
    "\n",
    "    return transforms.ToPILImage()(out.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Gradio app with two tabs\n",
    "with gr.Blocks(title=\"CycleGAN Translator\") as demo:\n",
    "    gr.Markdown(\"## CycleGAN: A ↔ B Image Translation\")\n",
    "\n",
    "    with gr.Tab(\"A → B\"):\n",
    "        inp_ab = gr.Image(label=\"Input A\", type=\"pil\")\n",
    "        out_ab = gr.Image(label=\"Generated B\")\n",
    "        btn_ab = gr.Button(\"Generate A→B\")\n",
    "        btn_ab.click(fn=translate_A2B, inputs=inp_ab, outputs=out_ab)\n",
    "\n",
    "    with gr.Tab(\"B → A\"):\n",
    "        inp_ba = gr.Image(label=\"Input B\", type=\"pil\")\n",
    "        out_ba = gr.Image(label=\"Generated A\")\n",
    "        btn_ba = gr.Button(\"Generate B→A\")\n",
    "        btn_ba.click(fn=translate_B2A, inputs=inp_ba, outputs=out_ba)\n",
    "\n",
    "    gr.Markdown(\"Upload an image and choose the tab for the direction you want.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
